scrapy startproject <project_name>

scrapy genspider <spider_name> <web_address>

scrapy runspider <spider_file>

scrapy runspider wikipedia.py -o articles.csv -t csv -s CLOSESPIDER_PAGECOUNT=10

wikipedia.org/robots.txt
# By default in settings.py ROBOTSTXT_OBEY = True
# It is useful to check robots.txt file, bcz sometimes it contains sitemaps
# For example http://edition.cnn.com/robots.txt contains several sitemaps in xml
# The purpose of sitemap xml files is to be read by search engine scrapers, so that search engines don't miss any pages
# For example https://www.cnn.com/sitemaps/cnn/index.xml contains other sidemaps where pages are grouped by month

pip install scrapy-selenium
# https://chromedriver.chromium.org/downloads

# scrapy code ==> scrapy-selenium ==> selenium ==> driver file ==> web browser

